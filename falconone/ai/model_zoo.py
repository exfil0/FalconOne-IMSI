"""
ML Model Zoo - v1.7.0
=====================
Centralized model registry and management for FalconOne ML components.

Features:
- Pre-trained model registry (signal classification, KPI prediction, etc.)
- Easy model loading with version management
- Model performance tracking
- Automatic model caching
"""

import os
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
import pickle


@dataclass
class ModelMetadata:
    """Model metadata and version info"""
    model_id: str
    name: str
    version: str
    task: str  # classification, prediction, generation
    framework: str  # tensorflow, pytorch, sklearn
    input_shape: Tuple
    output_shape: Tuple
    accuracy: float
    file_path: str
    file_size_mb: float
    created_at: str
    description: str = ""
    tags: List[str] = None
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []


class ModelZoo:
    """
    Centralized model registry for FalconOne.
    
    Manages pre-trained models across all ML components:
    - Signal classifier (CNN/LSTM/Transformer)
    - KPI predictor (LSTM)
    - SUCI deconcealment (neural network)
    - Crypto analyzer (ML-assisted)
    """
    
    def __init__(self, cache_dir: str = "/var/cache/falconone/models", logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger(__name__)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Model registry file
        self.registry_file = self.cache_dir / "model_registry.json"
        self.registry: Dict[str, ModelMetadata] = {}
        
        # Load existing registry
        self._load_registry()
        
        # Pre-define known models
        self._register_builtin_models()
        
        self.logger.info(f"[ModelZoo] Initialized with {len(self.registry)} models")
    
    def _load_registry(self):
        """Load model registry from disk."""
        if self.registry_file.exists():
            try:
                with open(self.registry_file, 'r') as f:
                    data = json.load(f)
                
                for model_id, model_data in data.items():
                    self.registry[model_id] = ModelMetadata(**model_data)
                
                self.logger.debug(f"[ModelZoo] Loaded {len(self.registry)} models from registry")
            except Exception as e:
                self.logger.error(f"[ModelZoo] Failed to load registry: {e}")
    
    def _save_registry(self):
        """Save model registry to disk."""
        try:
            data = {model_id: asdict(meta) for model_id, meta in self.registry.items()}
            
            with open(self.registry_file, 'w') as f:
                json.dump(data, f, indent=2)
            
            self.logger.debug("[ModelZoo] Registry saved")
        except Exception as e:
            self.logger.error(f"[ModelZoo] Failed to save registry: {e}")
    
    def _register_builtin_models(self):
        """Register built-in pre-trained models."""
        
        # Signal Classifier - CNN model
        if "signal_classifier_cnn_v1" not in self.registry:
            self.register_model(
                model_id="signal_classifier_cnn_v1",
                name="Signal Classifier CNN",
                version="1.0",
                task="classification",
                framework="tensorflow",
                input_shape=(1024, 2),  # IQ samples
                output_shape=(10,),  # 10 signal classes
                accuracy=0.94,
                file_path=str(self.cache_dir / "signal_classifier_cnn_v1.h5"),
                description="CNN-based signal classifier for GSM/CDMA/UMTS/LTE/5G/6G",
                tags=["signal", "classification", "cnn"]
            )
        
        # Signal Classifier - Transformer model
        if "signal_classifier_transformer_v1" not in self.registry:
            self.register_model(
                model_id="signal_classifier_transformer_v1",
                name="Signal Classifier Transformer",
                version="1.0",
                task="classification",
                framework="tensorflow",
                input_shape=(1024, 2),
                output_shape=(10,),
                accuracy=0.96,
                file_path=str(self.cache_dir / "signal_classifier_transformer_v1.h5"),
                description="Transformer-based signal classifier with attention mechanism",
                tags=["signal", "classification", "transformer", "attention"]
            )
        
        # KPI Predictor - LSTM model
        if "kpi_predictor_lstm_v1" not in self.registry:
            self.register_model(
                model_id="kpi_predictor_lstm_v1",
                name="KPI Predictor LSTM",
                version="1.0",
                task="prediction",
                framework="tensorflow",
                input_shape=(10, 5),  # 10 timesteps, 5 KPI features
                output_shape=(5,),  # Predict next 5 KPIs
                accuracy=0.88,
                file_path=str(self.cache_dir / "kpi_predictor_lstm_v1.h5"),
                description="LSTM for KPI time-series prediction",
                tags=["kpi", "prediction", "lstm", "timeseries"]
            )
        
        # SUCI Deconcealment - Neural network
        if "suci_deconcealer_v1" not in self.registry:
            self.register_model(
                model_id="suci_deconcealer_v1",
                name="SUCI Deconcealer",
                version="1.0",
                task="generation",
                framework="tensorflow",
                input_shape=(16,),  # Encrypted SUCI
                output_shape=(16,),  # Decrypted SUPI
                accuracy=0.75,
                file_path=str(self.cache_dir / "suci_deconcealer_v1.h5"),
                description="Neural network for SUCI concealment breaking (research use)",
                tags=["crypto", "suci", "5g", "security"]
            )
        
        # RIC Optimizer - RL model
        if "ric_optimizer_rl_v1" not in self.registry:
            self.register_model(
                model_id="ric_optimizer_rl_v1",
                name="RIC Optimizer RL",
                version="1.0",
                task="prediction",
                framework="tensorflow",
                input_shape=(20,),  # State space
                output_shape=(5,),  # Action space
                accuracy=0.82,
                file_path=str(self.cache_dir / "ric_optimizer_rl_v1.h5"),
                description="Reinforcement learning model for RAN optimization",
                tags=["ric", "optimization", "rl", "oran"]
            )
    
    def register_model(
        self,
        model_id: str,
        name: str,
        version: str,
        task: str,
        framework: str,
        input_shape: Tuple,
        output_shape: Tuple,
        accuracy: float,
        file_path: str,
        description: str = "",
        tags: Optional[List[str]] = None
    ) -> ModelMetadata:
        """
        Register a new model in the zoo.
        
        Args:
            model_id: Unique identifier
            name: Human-readable name
            version: Version string (e.g., "1.0", "2.1")
            task: Task type (classification, prediction, generation)
            framework: ML framework (tensorflow, pytorch, sklearn)
            input_shape: Input tensor shape
            output_shape: Output tensor shape
            accuracy: Model accuracy (0.0-1.0)
            file_path: Path to model file
            description: Model description
            tags: List of tags for filtering
            
        Returns:
            ModelMetadata instance
        """
        # Calculate file size
        file_size_mb = 0.0
        if os.path.exists(file_path):
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        
        metadata = ModelMetadata(
            model_id=model_id,
            name=name,
            version=version,
            task=task,
            framework=framework,
            input_shape=input_shape,
            output_shape=output_shape,
            accuracy=accuracy,
            file_path=file_path,
            file_size_mb=file_size_mb,
            created_at=datetime.utcnow().isoformat(),
            description=description,
            tags=tags or []
        )
        
        self.registry[model_id] = metadata
        self._save_registry()
        
        self.logger.info(f"[ModelZoo] Registered model: {name} v{version} ({model_id})")
        
        return metadata
    
    def load_model(self, model_id: str, lazy: bool = False) -> Optional[Any]:
        """
        Load model from registry.
        
        Args:
            model_id: Model identifier
            lazy: If True, only return metadata without loading model
            
        Returns:
            Loaded model or metadata (if lazy=True)
        """
        if model_id not in self.registry:
            self.logger.error(f"[ModelZoo] Model not found: {model_id}")
            return None
        
        metadata = self.registry[model_id]
        
        if lazy:
            return metadata
        
        # Load model based on framework
        try:
            if metadata.framework == 'tensorflow':
                return self._load_tensorflow_model(metadata)
            elif metadata.framework == 'pytorch':
                return self._load_pytorch_model(metadata)
            elif metadata.framework == 'sklearn':
                return self._load_sklearn_model(metadata)
            else:
                self.logger.error(f"[ModelZoo] Unsupported framework: {metadata.framework}")
                return None
        except Exception as e:
            self.logger.error(f"[ModelZoo] Failed to load model {model_id}: {e}")
            return None
    
    def _load_tensorflow_model(self, metadata: ModelMetadata) -> Optional[Any]:
        """Load TensorFlow/Keras model."""
        try:
            import tensorflow as tf
            
            if not os.path.exists(metadata.file_path):
                self.logger.warning(f"[ModelZoo] Model file not found: {metadata.file_path}")
                return None
            
            model = tf.keras.models.load_model(metadata.file_path)
            self.logger.info(f"[ModelZoo] Loaded TensorFlow model: {metadata.name}")
            return model
            
        except ImportError:
            self.logger.error("[ModelZoo] TensorFlow not installed")
            return None
        except Exception as e:
            self.logger.error(f"[ModelZoo] TensorFlow load error: {e}")
            return None
    
    def _load_pytorch_model(self, metadata: ModelMetadata) -> Optional[Any]:
        """Load PyTorch model."""
        try:
            import torch
            
            if not os.path.exists(metadata.file_path):
                self.logger.warning(f"[ModelZoo] Model file not found: {metadata.file_path}")
                return None
            
            model = torch.load(metadata.file_path)
            self.logger.info(f"[ModelZoo] Loaded PyTorch model: {metadata.name}")
            return model
            
        except ImportError:
            self.logger.error("[ModelZoo] PyTorch not installed")
            return None
        except Exception as e:
            self.logger.error(f"[ModelZoo] PyTorch load error: {e}")
            return None
    
    def _load_sklearn_model(self, metadata: ModelMetadata) -> Optional[Any]:
        """Load scikit-learn model."""
        try:
            if not os.path.exists(metadata.file_path):
                self.logger.warning(f"[ModelZoo] Model file not found: {metadata.file_path}")
                return None
            
            with open(metadata.file_path, 'rb') as f:
                model = pickle.load(f)
            
            self.logger.info(f"[ModelZoo] Loaded sklearn model: {metadata.name}")
            return model
            
        except Exception as e:
            self.logger.error(f"[ModelZoo] sklearn load error: {e}")
            return None
    
    def list_models(self, task: Optional[str] = None, framework: Optional[str] = None, tags: Optional[List[str]] = None) -> List[ModelMetadata]:
        """
        List models with optional filtering.
        
        Args:
            task: Filter by task type
            framework: Filter by framework
            tags: Filter by tags (any match)
            
        Returns:
            List of matching ModelMetadata
        """
        models = list(self.registry.values())
        
        if task:
            models = [m for m in models if m.task == task]
        
        if framework:
            models = [m for m in models if m.framework == framework]
        
        if tags:
            models = [m for m in models if any(tag in m.tags for tag in tags)]
        
        return models
    
    def get_model_info(self, model_id: str) -> Optional[ModelMetadata]:
        """Get model metadata by ID."""
        return self.registry.get(model_id)
    
    def delete_model(self, model_id: str, delete_file: bool = False):
        """
        Remove model from registry.
        
        Args:
            model_id: Model to remove
            delete_file: If True, also delete model file from disk
        """
        if model_id not in self.registry:
            self.logger.warning(f"[ModelZoo] Model not found: {model_id}")
            return
        
        metadata = self.registry[model_id]
        
        if delete_file and os.path.exists(metadata.file_path):
            try:
                os.remove(metadata.file_path)
                self.logger.info(f"[ModelZoo] Deleted model file: {metadata.file_path}")
            except Exception as e:
                self.logger.error(f"[ModelZoo] Failed to delete file: {e}")
        
        del self.registry[model_id]
        self._save_registry()
        
        self.logger.info(f"[ModelZoo] Removed model: {metadata.name}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get model zoo statistics."""
        total_size_mb = sum(m.file_size_mb for m in self.registry.values())
        
        by_task = {}
        for model in self.registry.values():
            by_task[model.task] = by_task.get(model.task, 0) + 1
        
        by_framework = {}
        for model in self.registry.values():
            by_framework[model.framework] = by_framework.get(model.framework, 0) + 1
        
        return {
            "total_models": len(self.registry),
            "total_size_mb": total_size_mb,
            "by_task": by_task,
            "by_framework": by_framework,
            "cache_dir": str(self.cache_dir)
        }
    
    def quantize_model(
        self,
        model_id: str,
        quantization_type: str = 'int8',
        optimize_for: str = 'latency'
    ) -> Optional[str]:
        """
        Quantize TensorFlow model to TFLite for edge deployment.
        
        Reduces model size by ~4x and increases inference speed by ~2-3x.
        
        Args:
            model_id: Model to quantize
            quantization_type: 'int8', 'float16', 'dynamic'
            optimize_for: 'latency', 'size', or 'default'
            
        Returns:
            Path to quantized .tflite model or None if failed
        """
        if model_id not in self.registry:
            self.logger.error(f"[ModelZoo] Model not found: {model_id}")
            return None
        
        metadata = self.registry[model_id]
        
        if metadata.framework != 'tensorflow':
            self.logger.error(f"[ModelZoo] Quantization only supports TensorFlow models")
            return None
        
        try:
            import tensorflow as tf
            
            # Load model
            model = self._load_tensorflow_model(metadata)
            if model is None:
                return None
            
            # Create converter
            converter = tf.lite.TFLiteConverter.from_keras_model(model)
            
            # Set optimization strategy
            if optimize_for == 'latency':
                converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]
            elif optimize_for == 'size':
                converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
            else:
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # Set quantization
            if quantization_type == 'int8':
                converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
                converter.inference_input_type = tf.int8
                converter.inference_output_type = tf.int8
                
                # Representative dataset for full integer quantization
                def representative_dataset():
                    import numpy as np
                    # Generate representative data based on input shape
                    for _ in range(100):
                        data = np.random.random(size=(1,) + metadata.input_shape).astype(np.float32)
                        yield [data]
                
                converter.representative_dataset = representative_dataset
                
            elif quantization_type == 'float16':
                converter.target_spec.supported_types = [tf.float16]
                
            elif quantization_type == 'dynamic':
                # Dynamic range quantization (weights only)
                pass  # Already set by optimizations
            
            # Convert model
            tflite_model = converter.convert()
            
            # Save quantized model
            tflite_path = metadata.file_path.replace('.h5', f'_{quantization_type}.tflite')
            with open(tflite_path, 'wb') as f:
                f.write(tflite_model)
            
            # Calculate size reduction
            original_size_mb = metadata.file_size_mb
            quantized_size_mb = os.path.getsize(tflite_path) / (1024 * 1024)
            size_reduction = ((original_size_mb - quantized_size_mb) / original_size_mb) * 100
            
            self.logger.info(
                f"[ModelZoo] Quantized {metadata.name}: "
                f"{original_size_mb:.2f}MB â†’ {quantized_size_mb:.2f}MB "
                f"({size_reduction:.1f}% reduction)"
            )
            
            # Register quantized model
            quantized_model_id = f"{model_id}_{quantization_type}"
            self.register_model(
                model_id=quantized_model_id,
                name=f"{metadata.name} ({quantization_type.upper()})",
                version=metadata.version,
                task=metadata.task,
                framework='tflite',
                input_shape=metadata.input_shape,
                output_shape=metadata.output_shape,
                accuracy=metadata.accuracy * 0.98,  # Slight accuracy loss
                file_path=tflite_path,
                description=f"Quantized version of {metadata.name} using {quantization_type}",
                tags=metadata.tags + ['quantized', quantization_type]
            )
            
            return tflite_path
            
        except ImportError:
            self.logger.error("[ModelZoo] TensorFlow not installed")
            return None
        except Exception as e:
            self.logger.error(f"[ModelZoo] Quantization failed: {e}")
            return None
    
    def load_tflite_model(self, model_id: str) -> Optional[Any]:
        """
        Load TFLite quantized model for inference.
        
        Returns TFLite Interpreter instance.
        """
        if model_id not in self.registry:
            self.logger.error(f"[ModelZoo] Model not found: {model_id}")
            return None
        
        metadata = self.registry[model_id]
        
        if metadata.framework != 'tflite':
            self.logger.error(f"[ModelZoo] Model is not TFLite format")
            return None
        
        try:
            import tensorflow as tf
            
            if not os.path.exists(metadata.file_path):
                self.logger.warning(f"[ModelZoo] Model file not found: {metadata.file_path}")
                return None
            
            # Create interpreter
            interpreter = tf.lite.Interpreter(model_path=metadata.file_path)
            interpreter.allocate_tensors()
            
            self.logger.info(f"[ModelZoo] Loaded TFLite model: {metadata.name}")
            
            return interpreter
            
        except ImportError:
            self.logger.error("[ModelZoo] TensorFlow not installed")
            return None
        except Exception as e:
            self.logger.error(f"[ModelZoo] TFLite load error: {e}")
            return None
    
    def benchmark_model(self, model_id: str, num_runs: int = 100) -> Dict[str, Any]:
        """
        Benchmark model inference performance.
        
        Args:
            model_id: Model to benchmark
            num_runs: Number of inference runs
            
        Returns:
            Performance metrics (latency, throughput)
        """
        if model_id not in self.registry:
            self.logger.error(f"[ModelZoo] Model not found: {model_id}")
            return {}
        
        metadata = self.registry[model_id]
        
        try:
            import time
            import numpy as np
            
            # Load model
            if metadata.framework == 'tflite':
                model = self.load_tflite_model(model_id)
                if model is None:
                    return {}
                
                # Get input details
                input_details = model.get_input_details()
                output_details = model.get_output_details()
                
                # Benchmark TFLite
                latencies = []
                for _ in range(num_runs):
                    # Generate random input
                    input_data = np.random.random(size=input_details[0]['shape']).astype(np.float32)
                    
                    # Inference
                    start = time.time()
                    model.set_tensor(input_details[0]['index'], input_data)
                    model.invoke()
                    _ = model.get_tensor(output_details[0]['index'])
                    latency = (time.time() - start) * 1000  # ms
                    latencies.append(latency)
                
            else:
                model = self.load_model(model_id)
                if model is None:
                    return {}
                
                # Benchmark regular model
                latencies = []
                for _ in range(num_runs):
                    # Generate random input
                    input_data = np.random.random(size=(1,) + metadata.input_shape).astype(np.float32)
                    
                    # Inference
                    start = time.time()
                    _ = model.predict(input_data, verbose=0)
                    latency = (time.time() - start) * 1000  # ms
                    latencies.append(latency)
            
            # Calculate statistics
            avg_latency = np.mean(latencies)
            p50_latency = np.percentile(latencies, 50)
            p95_latency = np.percentile(latencies, 95)
            p99_latency = np.percentile(latencies, 99)
            throughput = 1000.0 / avg_latency  # inferences/sec
            
            result = {
                "model_id": model_id,
                "num_runs": num_runs,
                "avg_latency_ms": avg_latency,
                "p50_latency_ms": p50_latency,
                "p95_latency_ms": p95_latency,
                "p99_latency_ms": p99_latency,
                "throughput_per_sec": throughput,
                "model_size_mb": metadata.file_size_mb
            }
            
            self.logger.info(
                f"[ModelZoo] Benchmark {metadata.name}: "
                f"{avg_latency:.2f}ms avg, {throughput:.1f} inferences/sec"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"[ModelZoo] Benchmark failed: {e}")
            return {}

